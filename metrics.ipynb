{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference for 1-0843x4.png:  10.018514662502168\n",
      "Mean difference for 10-lena.png:  22.375782012939453\n",
      "Mean difference for 8-ADE20K-1C.png:  26.452731481481482\n",
      "Mean difference for 17-1194.png:  45.323030555555555\n",
      "Mean difference for 2-0868x4.png:  11.694742321707444\n",
      "Mean difference for 29-img_5182.png:  16.489420168067227\n",
      "Mean difference for 27-img_5264.png:  19.661710084033615\n",
      "Mean difference for 6-elephant_3.png:  8.48270034790039\n",
      "Mean difference for 24-2010_002838.png:  20.16920094562648\n",
      "Mean difference for 20-2009_003829.png:  26.78303592814371\n",
      "Mean difference for 16-P1020854.png:  21.914449869791667\n",
      "Mean difference for 25-2008_002622.png:  15.015391549295774\n",
      "Mean difference for 14-comic.png:  55.69631024930748\n",
      "Mean difference for 13-cameraman.png:  17.549687430385386\n",
      "Mean difference for 22-335094.png:  37.48322225892319\n",
      "Mean difference for 15-tire.png:  16.111541748046875\n",
      "Mean difference for 7-CITYSCAPES-2C.png:  17.355756172839506\n",
      "Mean difference for 28-img_043_SRF_2_HR.png:  36.171700613839285\n",
      "Mean difference for 30-167062.png:  10.150957571518319\n",
      "Mean difference for 4-0896x4.png:  12.153542715021112\n",
      "Mean difference for 11-NYUD-2.png:  26.096508403361344\n",
      "Mean difference for 9-MDBD-1C.png:  20.810976080246913\n",
      "Mean difference for 18-img_5935.png:  24.423361344537817\n",
      "Mean difference for 12-BIPED-1C.png:  36.14300347222222\n",
      "Mean difference for 21-00065305.png:  21.33326\n",
      "Mean difference for 3-35028.png:  16.083866037137064\n",
      "Mean difference for 19-img_6150.png:  18.62590756302521\n",
      "Mean difference for 5-WIREFRAME-2.png:  16.721855421686747\n",
      "Mean difference for 26-P1020177.png:  34.22393880208333\n",
      "Mean difference for 23-img_011_SRF_2_HR.png:  55.43455646054964\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the paths to the folders\n",
    "ground_truth_folder = '/Users/jamesdarby/Documents/McGill/COMP 551/Asg4/UDED/gt'\n",
    "result_folder = '/Users/jamesdarby/Documents/McGill/COMP 551/Asg4/TEED/result/BIPED2UDED/fused'\n",
    "\n",
    "# Get a list of filenames in the ground truth folder\n",
    "ground_truth_images = os.listdir(ground_truth_folder)\n",
    "\n",
    "# Loop through each file in the ground truth folder\n",
    "for filename in ground_truth_images:\n",
    "    if filename.endswith('.png'):\n",
    "        # Construct the full file paths\n",
    "        gt_path = os.path.join(ground_truth_folder, filename)\n",
    "        result_path = os.path.join(result_folder, filename)\n",
    "\n",
    "        # Read the images\n",
    "        gt_image = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED)\n",
    "        result_image = cv2.imread(result_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if result_image is not None:\n",
    "            # Invert the image\n",
    "            inverted_image = 255 - result_image\n",
    "\n",
    "        # Check if both images are loaded\n",
    "        if gt_image is not None and result_image is not None:\n",
    "            # Resize result image to match ground truth image if necessary\n",
    "            if gt_image.shape != result_image.shape:\n",
    "                result_image = cv2.resize(result_image, (gt_image.shape[1], gt_image.shape[0]))\n",
    "\n",
    "            inv_diff_img = cv2.absdiff(gt_image, inverted_image)\n",
    "\n",
    "            # Process the difference here (e.g., calculate statistics or save the image)\n",
    "            # For example, to save the difference image:\n",
    "            # diff_path = os.path.join('path/to/difference_folder', filename)\n",
    "            # cv2.imwrite(diff_path, difference)\n",
    "\n",
    "            # To calculate and print mean difference\n",
    "            inv_diff = np.mean(inv_diff_img)\n",
    "            print(f\"Mean difference for {filename}:  {inv_diff}\")\n",
    "\n",
    "# Add any additional processing as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE Ratio over dataset: 0.06343134376401705\n",
      "Average MAE Ratio over dataset: 0.09371904161241319\n"
     ]
    }
   ],
   "source": [
    "# Initialize accumulators for MSE and MAE\n",
    "total_mse = 0.0\n",
    "total_mae = 0.0\n",
    "num_images = 0\n",
    "\n",
    "# Get a list of filenames in the ground truth folder\n",
    "ground_truth_images = os.listdir(ground_truth_folder)\n",
    "\n",
    "# Loop through each file in the ground truth folder\n",
    "for filename in ground_truth_images:\n",
    "    if filename.endswith('.png'):\n",
    "        # Increment the image counter\n",
    "        num_images += 1\n",
    "\n",
    "        # Construct the full file paths\n",
    "        gt_path = os.path.join(ground_truth_folder, filename)\n",
    "        result_path = os.path.join(result_folder, filename)\n",
    "\n",
    "        # Read the images\n",
    "        gt_image = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "        result_image = cv2.imread(result_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "\n",
    "        # Check if both images are loaded\n",
    "        if gt_image is not None and result_image is not None:\n",
    "            # Resize result image to match ground truth image if necessary\n",
    "            if gt_image.shape != result_image.shape:\n",
    "                result_image = cv2.resize(result_image, (gt_image.shape[1], gt_image.shape[0]))\n",
    "\n",
    "            # Invert the result image\n",
    "            inverted_image = 255 - result_image\n",
    "\n",
    "            # Calculate squared differences and absolute differences\n",
    "            squared_diff = (gt_image - inverted_image) ** 2\n",
    "            absolute_diff = np.abs(gt_image - inverted_image)\n",
    "\n",
    "            # Accumulate MSE and MAE\n",
    "            total_mse += np.mean(squared_diff)\n",
    "            total_mae += np.mean(absolute_diff)\n",
    "\n",
    "# Calculate average MSE and MAE\n",
    "average_mse = total_mse / num_images\n",
    "average_mae = total_mae / num_images\n",
    "\n",
    "# Normalize the average errors\n",
    "average_mse_ratio = average_mse / 255 ** 2\n",
    "average_mae_ratio = average_mae / 255\n",
    "\n",
    "print(f\"Average MSE Ratio over dataset: {average_mse_ratio}\")\n",
    "print(f\"Average MAE Ratio over dataset: {average_mae_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def bdcn_loss2(inputs, targets, l_weight=1.1):\n",
    "    # bdcn loss modified in DexiNed\n",
    "\n",
    "    targets = targets.long()\n",
    "    mask = targets.float()\n",
    "    num_positive = torch.sum((mask > 0.0).float()).float() # >0.1\n",
    "    num_negative = torch.sum((mask <= 0.0).float()).float() # <= 0.1\n",
    "\n",
    "    mask[mask > 0.] = 1.0 * num_negative / (num_positive + num_negative) #0.1\n",
    "    mask[mask <= 0.] = 1.1 * num_positive / (num_positive + num_negative)  # before mask[mask <= 0.1]\n",
    "    inputs= torch.sigmoid(inputs)\n",
    "    cost = torch.nn.BCELoss(mask, reduction='none')(inputs, targets.float())\n",
    "    cost = torch.sum(cost.float().mean((1, 2, 3))) # before sum\n",
    "    return l_weight*cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for 1-0843x4.png: 0.04150024428963661\n",
      "Loss for 10-lena.png: 0.08813481777906418\n",
      "Loss for 8-ADE20K-1C.png: 0.11825072765350342\n",
      "Loss for 17-1194.png: 0.21944889426231384\n",
      "Loss for 2-0868x4.png: 0.06642170250415802\n",
      "Loss for 29-img_5182.png: 0.07291970402002335\n",
      "Loss for 27-img_5264.png: 0.09982876479625702\n",
      "Loss for 6-elephant_3.png: 0.032409779727458954\n",
      "Loss for 24-2010_002838.png: 0.09917829185724258\n",
      "Loss for 20-2009_003829.png: 0.14548587799072266\n",
      "Loss for 16-P1020854.png: 0.13548116385936737\n",
      "Loss for 25-2008_002622.png: 0.074457548558712\n",
      "Loss for 14-comic.png: 0.27993443608283997\n",
      "Loss for 13-cameraman.png: 0.05600902438163757\n",
      "Loss for 22-335094.png: 0.17815403640270233\n",
      "Loss for 15-tire.png: 0.07857025414705276\n",
      "Loss for 7-CITYSCAPES-2C.png: 0.08494852483272552\n",
      "Loss for 28-img_043_SRF_2_HR.png: 0.2061677873134613\n",
      "Loss for 30-167062.png: 0.035149212926626205\n",
      "Loss for 4-0896x4.png: 0.06447357684373856\n",
      "Loss for 11-NYUD-2.png: 0.12742844223976135\n",
      "Loss for 9-MDBD-1C.png: 0.06806842982769012\n",
      "Loss for 18-img_5935.png: 0.10998158901929855\n",
      "Loss for 12-BIPED-1C.png: 0.16398392617702484\n",
      "Loss for 21-00065305.png: 0.08991315215826035\n",
      "Loss for 3-35028.png: 0.085329070687294\n",
      "Loss for 19-img_6150.png: 0.08962440490722656\n",
      "Loss for 5-WIREFRAME-2.png: 0.0969429463148117\n",
      "Loss for 26-P1020177.png: 0.20303024351596832\n",
      "Loss for 23-img_011_SRF_2_HR.png: 0.32061880826950073\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loop through each file in the ground truth folder\n",
    "for filename in ground_truth_images:\n",
    "    if filename.endswith('.png'):\n",
    "        # Construct the full file paths\n",
    "        gt_path = os.path.join(ground_truth_folder, filename)\n",
    "        result_path = os.path.join(result_folder, filename)\n",
    "\n",
    "        # Read the images\n",
    "        gt_image = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.0\n",
    "        result_image = cv2.imread(result_path, cv2.IMREAD_UNCHANGED).astype(np.float32) / 255.0\n",
    "\n",
    "        # Check if both images are loaded\n",
    "        if gt_image is not None and result_image is not None:\n",
    "            # Resize result image to match ground truth image if necessary\n",
    "            if gt_image.shape != result_image.shape:\n",
    "                result_image = cv2.resize(result_image, (gt_image.shape[1], gt_image.shape[0]))\n",
    "\n",
    "            # Convert images to PyTorch tensors\n",
    "            gt_tensor = torch.tensor(gt_image).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "            result_tensor = torch.tensor(result_image).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # Calculate the bdcn loss\n",
    "            loss = bdcn_loss2(result_tensor, gt_tensor)\n",
    "            print(f\"Loss for {filename}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ cats losses ----------\n",
    "def bdrloss(prediction, label, radius,device='cpu'):\n",
    "    '''\n",
    "    The boundary tracing loss that handles the confusing pixels.\n",
    "    '''\n",
    "\n",
    "    filt = torch.ones(1, 1, 2*radius+1, 2*radius+1)\n",
    "    filt.requires_grad = False\n",
    "    filt = filt.to(device)\n",
    "\n",
    "    bdr_pred = prediction * label\n",
    "    pred_bdr_sum = label * F.conv2d(bdr_pred, filt, bias=None, stride=1, padding=radius)\n",
    "\n",
    "    texture_mask = F.conv2d(label.float(), filt, bias=None, stride=1, padding=radius)\n",
    "    mask = (texture_mask != 0).float()\n",
    "    mask[label == 1] = 0\n",
    "    pred_texture_sum = F.conv2d(prediction * (1-label) * mask, filt, bias=None, stride=1, padding=radius)\n",
    "\n",
    "    softmax_map = torch.clamp(pred_bdr_sum / (pred_texture_sum + pred_bdr_sum + 1e-10), 1e-10, 1 - 1e-10)\n",
    "    cost = -label * torch.log(softmax_map)\n",
    "    cost[label == 0] = 0\n",
    "\n",
    "    return torch.sum(cost.float().mean((1, 2, 3)))\n",
    "\n",
    "def textureloss(prediction, label, mask_radius, device='cpu'):\n",
    "    '''\n",
    "    The texture suppression loss that smooths the texture regions.\n",
    "    '''\n",
    "    filt1 = torch.ones(1, 1, 3, 3)\n",
    "    filt1.requires_grad = False\n",
    "    filt1 = filt1.to(device)\n",
    "    filt2 = torch.ones(1, 1, 2*mask_radius+1, 2*mask_radius+1)\n",
    "    filt2.requires_grad = False\n",
    "    filt2 = filt2.to(device)\n",
    "\n",
    "    pred_sums = F.conv2d(prediction.float(), filt1, bias=None, stride=1, padding=1)\n",
    "    label_sums = F.conv2d(label.float(), filt2, bias=None, stride=1, padding=mask_radius)\n",
    "\n",
    "    mask = 1 - torch.gt(label_sums, 0).float()\n",
    "\n",
    "    loss = -torch.log(torch.clamp(1-pred_sums/9, 1e-10, 1-1e-10))\n",
    "    loss[mask == 0] = 0\n",
    "\n",
    "    return torch.sum(loss.float().mean((1, 2, 3)))\n",
    "\n",
    "def cats_loss(prediction, label, l_weight=[0.,0.], device='cpu'):\n",
    "    # tracingLoss\n",
    "\n",
    "    tex_factor,bdr_factor = l_weight\n",
    "    balanced_w = 1.1\n",
    "    label = label.float()\n",
    "    prediction = prediction.float()\n",
    "    with torch.no_grad():\n",
    "        mask = label.clone()\n",
    "\n",
    "        num_positive = torch.sum((mask == 1).float()).float()\n",
    "        num_negative = torch.sum((mask == 0).float()).float()\n",
    "        beta = num_negative / (num_positive + num_negative)\n",
    "        mask[mask == 1] = beta\n",
    "        mask[mask == 0] = balanced_w * (1 - beta)\n",
    "        mask[mask == 2] = 0\n",
    "\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "\n",
    "    cost = torch.nn.functional.binary_cross_entropy(\n",
    "        prediction.float(), label.float(), weight=mask, reduction='none')\n",
    "    cost = torch.sum(cost.float().mean((1, 2, 3)))  # by me\n",
    "    label_w = (label != 0).float()\n",
    "    textcost = textureloss(prediction.float(), label_w.float(), mask_radius=4, device=device)\n",
    "    bdrcost = bdrloss(prediction.float(), label_w.float(), radius=4, device=device)\n",
    "\n",
    "    return cost + bdr_factor * bdrcost + tex_factor * textcost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
